


pip install tensorflow keras mne scikit-learn


import os
import numpy as np
import random
import tensorflow as tf
import matplotlib.pyplot as plt
import mne
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Conv2D, DepthwiseConv2D, SeparableConv2D, AveragePooling2D,
                                     BatchNormalization, Activation, Dropout, Flatten, Dense)
from tensorflow.keras.constraints import max_norm
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping

# Set seeds
np.random.seed(42)
random.seed(42)
tf.random.set_seed(42)

# === LOAD EEG DATA ===
data_dir = "../data"
edf_files = ["s001r04.edf", "s001r08.edf", "s002r04.edf"]
event_id = {"T1": 1, "T2": 2}
tmin, tmax = 0.0, 4.0
montage = mne.channels.make_standard_montage("standard_1020")

X_list = []
y_list = []

for file in edf_files:
    raw = mne.io.read_raw_edf(os.path.join(data_dir, file), preload=True)
    events, _ = mne.events_from_annotations(raw, event_id=event_id)
    if len(events) == 0:
        continue
    epochs = mne.Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True)
    epochs.rename_channels(lambda ch: ch.strip('.').upper())
    drop_chs = [ch for ch in epochs.info['ch_names'] if ch not in montage.ch_names]
    epochs.drop_channels(drop_chs)
    epochs.set_montage(montage)

    X_list.append(epochs.get_data())  # (epochs, channels, time)
    y_list.append(epochs.events[:, -1])

X = np.vstack(X_list)  # (samples, channels, time)
y = np.hstack(y_list)
y = np.array([0 if label == 1 else 1 for label in y])  # Binary class

# === PREPROCESS FOR EEGNet ===
X = X[:, :, :, np.newaxis]             # (samples, channels, time, 1)
X = np.transpose(X, (0, 2, 1, 3))      # (samples, time, channels, 1)
print("Final EEGNet input shape:", X.shape)

# === DEFINE EEGNet ===
def EEGNet(nb_classes, Chans=52, Samples=641, dropoutRate=0.5, kernLength=64,
           F1=8, D=2, F2=16, norm_rate=0.25):
    input1 = Input(shape=(Samples, Chans, 1))

    x = Conv2D(F1, (kernLength, 1), padding='same', use_bias=False, data_format='channels_last')(input1)
    x = BatchNormalization()(x)
    x = DepthwiseConv2D((1, Chans), use_bias=False, depth_multiplier=D,
                        depthwise_constraint=max_norm(1.), data_format='channels_last')(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = AveragePooling2D((4, 1), data_format='channels_last')(x)
    x = Dropout(dropoutRate)(x)

    x = SeparableConv2D(F2, (16, 1), use_bias=False, padding='same', data_format='channels_last')(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = AveragePooling2D((8, 1), data_format='channels_last')(x)
    x = Dropout(dropoutRate)(x)

    x = Flatten()(x)
    x = Dense(nb_classes, activation='softmax', kernel_constraint=max_norm(norm_rate))(x)

    return Model(inputs=input1, outputs=x)

# === COMPILE & TRAIN ===
y_cat = to_categorical(y)
X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.3, stratify=y, random_state=42)

model = EEGNet(nb_classes=2, Chans=X.shape[2], Samples=X.shape[1])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
history = model.fit(X_train, y_train, batch_size=8, epochs=100, validation_split=0.2, callbacks=[early_stop], verbose=1)

# === PLOT ACCURACY & LOSS ===
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title("Accuracy")
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("Loss")
plt.legend()
plt.show()

# === EVALUATE ON TEST SET ===
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print("=== Classification Report ===")
print(classification_report(y_true, y_pred_classes))
print("Confusion Matrix:")
print(confusion_matrix(y_true, y_pred_classes))





# EEGNet Deep Learning Setup

import os
import numpy as np
import random
import tensorflow as tf
import mne
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt

# Set seeds for reproducibility
np.random.seed(42)
random.seed(42)
tf.random.set_seed(42)


# === Configuration ===
data_dir = "../data"
edf_files = ["s001r04.edf", "s001r08.edf", "s002r04.edf"]
event_id = {"T1": 1, "T2": 2}
tmin, tmax = 0.0, 4.0
montage = mne.channels.make_standard_montage("standard_1020")

X_list = []
y_list = []

for file in edf_files:
    raw = mne.io.read_raw_edf(os.path.join(data_dir, file), preload=True)
    events, _ = mne.events_from_annotations(raw, event_id=event_id)
    if len(events) == 0:
        continue

    epochs = mne.Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True)

    # Clean and filter channels
    epochs.rename_channels(lambda ch: ch.strip('.').upper())
    drop_chs = [ch for ch in epochs.info['ch_names'] if ch not in montage.ch_names]
    epochs.drop_channels(drop_chs)
    epochs.set_montage(montage)

    X_list.append(epochs.get_data())                      # shape: (epochs, channels, time)
    y_list.append(epochs.events[:, -1])                   # labels

X = np.vstack(X_list)         # shape: (samples, channels, time)
y = np.hstack(y_list)

# Convert labels to binary (0: T1, 1: T2)
y = np.array([0 if label == 1 else 1 for label in y])    # 0 = Left hand, 1 = Right hand

# Reshape to EEGNet input shape: (samples, 1, channels, time)
X = X[:, np.newaxis, :, :]     # shape: (samples, 1, channels, time)

# Final shape check
print("X shape:", X.shape)
print("y shape:", y.shape)





from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, SeparableConv2D, AveragePooling2D
from tensorflow.keras.layers import BatchNormalization, Activation, Dropout, Flatten, Dense
from tensorflow.keras.constraints import max_norm

def EEGNet(nb_classes, Chans=52, Samples=641, dropoutRate=0.5, kernLength=64,
           F1=8, D=2, F2=16, norm_rate=0.25):

    input1 = Input(shape=(1, Chans, Samples))

    # Block 1: Temporal Conv → Depthwise Spatial Conv
    x = Conv2D(F1, (1, kernLength), padding='same',
               input_shape=(1, Chans, Samples),
               use_bias=False)(input1)
    x = BatchNormalization()(x)
    x = DepthwiseConv2D((Chans, 1), use_bias=False,
                        depth_multiplier=D,
                        depthwise_constraint=max_norm(1.))(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = AveragePooling2D((1, 4))(x)
    x = Dropout(dropoutRate)(x)

    # Block 2: SeparableConv → Spatial summarization
    x = SeparableConv2D(F2, (1, 16), use_bias=False, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = AveragePooling2D((1, 8))(x)
    x = Dropout(dropoutRate)(x)

    # Classification block
    x = Flatten(name='flatten')(x)
    x = Dense(nb_classes, activation='softmax',
              kernel_constraint=max_norm(norm_rate))(x)

    return Model(inputs=input1, outputs=x)



from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping

# One-hot encode labels for softmax
y_cat = to_categorical(y)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.3, stratify=y, random_state=42)

# Build EEGNet model
model = EEGNet(nb_classes=2, Chans=X.shape[2], Samples=X.shape[3])
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Show model summary
model.summary()


# Train with early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = model.fit(X_train, y_train,
                    batch_size=8,
                    epochs=100,
                    validation_split=0.2,
                    callbacks=[early_stop],
                    verbose=1)



# Accuracy and loss curves
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title("Accuracy")
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("Loss")
plt.legend()
plt.show()




